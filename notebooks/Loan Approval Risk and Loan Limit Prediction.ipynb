{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Loan Approval Risk Prediction Using Machine Learning"
      ],
      "metadata": {
        "id": "vAtQPsqS1Zn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Business Problem\n",
        "\n",
        "Financial institutions must determine whether loan applicants should be approved or declined while minimising default risk.\n",
        "\n",
        "This project builds predictive classification models to identify high-risk applicants using historical loan data.\n"
      ],
      "metadata": {
        "id": "akbFSWHlfPjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Overview\n",
        "\n",
        "The dataset contains approximately 58,600 loan applications with financial and demographic attributes such as income, employment length, loan amount, interest rate, and credit history.\n",
        "\n",
        "The target variable is `loan_approval_status`, indicating whether a loan was approved or declined.\n",
        "\n",
        "Initial analysis shows the dataset is imbalanced, with approved loans significantly outnumbering declined loans.\n"
      ],
      "metadata": {
        "id": "Tp14gQV0fRvT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0H_lgxCgRWEi"
      },
      "outputs": [],
      "source": [
        "# Standard core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Model Selection and preprocessing\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Visualisation\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Models\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import RocCurveDisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "DATA_PATH = Path(\"../data/raw/loan_approval_data.csv\")\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "df.describe(include=\"all\").transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "japVVkzZWXqG",
        "outputId": "27f70d53-12a9-4541-de4b-f1ee530bcbd7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/raw/loan_approval_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-843815227.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mDATA_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/raw/loan_approval_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/raw/loan_approval_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check number of features and instances\n",
        "df.shape"
      ],
      "metadata": {
        "id": "FWPGFTsLyruv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset structure\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "LGcGThFRyrxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping the features we don't want\n",
        "df.drop(['id', 'max_allowed_loan', 'Credit_Application_Acceptance'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "ea7CyJQGyrzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the new dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "8CJH6CZbyr1o",
        "outputId": "4e57f694-9ec4-4e64-fffd-480e84af4449"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3198176213.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Display the new dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Display basic stats since we dropped features\n",
        "df.describe(include='all').transpose()"
      ],
      "metadata": {
        "id": "aNA2Jwjchn3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "PNlmdgKum8AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "RlynJp5em9Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Exploratory Data Analysis\n",
        "\n",
        "Before training models, exploratory analysis was performed to understand the structure and distribution of key variables.\n",
        "\n",
        "In particular, the distribution of the target variable (`loan_approval_status`) was examined to assess class imbalance, which has implications for model evaluation.\n"
      ],
      "metadata": {
        "id": "qIvoxWpYjJtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['loan_approval_status'].value_counts().plot(kind='bar')\n",
        "plt.title('Distribution of Target Variable (Loan Approval Status)')\n",
        "plt.xlabel('Loan Status'), plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a3bXb1uNeBKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "hAnB_bCOeBPN",
        "outputId": "bf9d4c86-0c9f-4701-b3d8-2ecf73ade6b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-964094849.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "Z99D3GKOeBRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='all').transpose()"
      ],
      "metadata": {
        "id": "mH3Q1AsvZ8Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "964AsYfZt5gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['age'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "J6PvQRQQFFXD",
        "outputId": "72ce0f52-678d-464a-c6f9-d3cbf7c02d94"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2262461417.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sex'].value_counts()"
      ],
      "metadata": {
        "id": "e7NEqlBLERgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Education_Qualifications'].value_counts()"
      ],
      "metadata": {
        "id": "dYIU9Vw2FU8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['income'].value_counts()\n",
        "#max(df['income'])\n",
        "#min(df['income'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "wr0hWKTbFdZX",
        "outputId": "edc786bc-31cb-4981-a4e0-1ee2e7b90c0d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1967294730.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'income'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#max(df['income'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#min(df['income'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['home_ownership'].value_counts()"
      ],
      "metadata": {
        "id": "1Kzs4Q-mERii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['emplyment_length'].value_counts()"
      ],
      "metadata": {
        "id": "RONJEHIOERk0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "1bfcce5c-3de6-4071-a435-1a49cf582263"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1891568682.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emplyment_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['loan_intent'].value_counts()"
      ],
      "metadata": {
        "id": "8uK806b4ERm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['loan_amount'].value_counts()"
      ],
      "metadata": {
        "id": "t6HQUnk5ERpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['loan_interest_rate'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "qr25eAXoeBT3",
        "outputId": "d2b50c96-fbb2-4857-e42b-ac1ba38b8b73"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-652507876.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loan_interest_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['loan_income_ratio'].value_counts()"
      ],
      "metadata": {
        "id": "u8-VOtKzEQ2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "8afc346c-1fb4-4b1a-aad8-adf8a757075b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-441425733.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loan_income_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['payment_default_on_file'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "s-5bMXGRqpKo",
        "outputId": "3b099b04-146c-4784-bb54-28e79804ab64"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2580348726.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'payment_default_on_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['credit_history_length'].value_counts()"
      ],
      "metadata": {
        "id": "7RVpzNxzEUIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['loan_approval_status'].value_counts()"
      ],
      "metadata": {
        "id": "-bPrCqMoEQ4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "2ROJJe3c3jUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes\n",
        "#df.describe()"
      ],
      "metadata": {
        "id": "rh-wkhiJEQ89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable age before cleaning\n",
        "#df['age'].describe()\n",
        "#df['age'].min(), df['age'].max()\n",
        "df['age'].value_counts()\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "UFwjjCu4pH68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting valid values to float and invalid values to NaN\n",
        "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
        "\n",
        "# Setting a range to find all unrealistic numbers (such as -30 and 156) and changing them to NaN\n",
        "df.loc[(df['age']<18) | (df['age']>95),'age'] = None\n",
        "\n",
        "# Drop NaN values\n",
        "df = df.dropna(subset=['age'])\n",
        "\n",
        "# Variable age after cleaning.\n",
        "df['age'].describe(include='all').round(2)\n",
        "\n",
        "#df['age'].isna().sum()"
      ],
      "metadata": {
        "id": "6AsfHsK7EQ_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['age'].dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "45YU1YP_eBWP",
        "outputId": "ed15bb11-5bab-4bf7-c928-07380602e7fd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1706844351.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable Sex before cleaning\n",
        "df['Sex'].describe()"
      ],
      "metadata": {
        "id": "0PZjZj_d3pux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable Sex after cleaning (drop column)\n",
        "df=df.drop(columns=['Sex'], axis =1)\n",
        "df.columns"
      ],
      "metadata": {
        "id": "iCBVTKio3pzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable Education_Qualifications before cleaning\n",
        "df['Education_Qualifications'].describe()"
      ],
      "metadata": {
        "id": "6_laTyLt3p3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable Education_Qualifications after cleaning (drop column)\n",
        "df=df.drop(columns=['Education_Qualifications'], axis =1)\n",
        "df.columns"
      ],
      "metadata": {
        "id": "27sYg8Gp3p7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable home_ownership before cleaning\n",
        "df.filter(like='home_ownership')"
      ],
      "metadata": {
        "id": "UClNXbVv3p9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable home_ownership after cleaning (one-hot encoding)\n",
        "df = pd.get_dummies(df, columns=['home_ownership'])\n",
        "\n",
        "df.filter(like='home_ownership')\n"
      ],
      "metadata": {
        "id": "TmlgRwm-3qCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable emplyment_length before cleaning\n",
        "df['emplyment_length'].value_counts().sort_index().tail(10)\n",
        "df['emplyment_length'].describe().round(2)"
      ],
      "metadata": {
        "id": "ZVTnA8Fj_T8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable emplyment_length after cleaning (setting all values less than 0 and above 75 to NaN, and then removing them)\n",
        "\n",
        "df.loc[(df['emplyment_length']<0) | (df['emplyment_length']>75), 'emplyment_length'] = None\n",
        "\n",
        "df = df.dropna(subset=['emplyment_length'])\n",
        "\n",
        "df['emplyment_length'].describe().round(2)\n",
        "\n",
        "#df['emplyment_length'].isna().sum()"
      ],
      "metadata": {
        "id": "4tCE3MOh_T_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable loan_intent before cleaning\n",
        "df.filter(like='loan_intent')"
      ],
      "metadata": {
        "id": "1hnTRBVJ_UCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable loan_intent after cleaning (one-hot encoding)\n",
        "df = pd.get_dummies(df, columns=['loan_intent'])\n",
        "\n",
        "df.filter(like='loan_intent')"
      ],
      "metadata": {
        "id": "G3tPiQ2Q_UFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable loan_interest_rate before cleaning\n",
        "df['loan_interest_rate'].describe().round(2)\n",
        "df['loan_interest_rate'].isna().sum()"
      ],
      "metadata": {
        "id": "ZC2CCllO_UI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable loan_interest_rate after cleaning (setting all values less than 0 and above 50 to NaN, and then removing them)\n",
        "df.loc[(df['loan_interest_rate']<0) | (df['loan_interest_rate']>50), 'loan_interest_rate'] = None\n",
        "\n",
        "df = df.dropna(subset=['loan_interest_rate'])\n",
        "\n",
        "df['loan_interest_rate'].describe().round(2)\n",
        "df['loan_interest_rate'].isna().sum()"
      ],
      "metadata": {
        "id": "OLdfWGrA_ULr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable payment_default_on_file before cleaning\n",
        "df['payment_default_on_file'].value_counts(dropna=False)\n",
        "df['payment_default_on_file'].describe()\n",
        "df.filter(like='payment_default_on_file')"
      ],
      "metadata": {
        "id": "xNk7lgBr_UNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable payment_default_on_file after cleaning\n",
        "#(standardising other values to either Y, or N)\n",
        "df['payment_default_on_file']=df['payment_default_on_file'].replace({'YES': 'Y','NO': 'N'})\n",
        "\n",
        "# Remove NaN values\n",
        "df = df.dropna(subset=['payment_default_on_file'])\n",
        "\n",
        "# Encode categorical values so they are binary.\n",
        "df['payment_default_on_file']=df['payment_default_on_file'].map({'Y':1, 'N':0})\n",
        "\n",
        "df.filter(like='payment_default_on_file')\n",
        "\n",
        "df['payment_default_on_file'].value_counts(dropna=False)\n",
        "\n",
        "#df['payment_default_on_file'].isna().sum()"
      ],
      "metadata": {
        "id": "lsBKwatk_UQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable loan_approval_status before cleaning\n",
        "df['loan_approval_status'].value_counts(dropna=False)"
      ],
      "metadata": {
        "id": "F_4kI6Hj_USl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable loan_approval_status after cleaning\n",
        "\n",
        "# Make all values lowercase and removing any unwanted characters from the before and after the word.\n",
        "df['loan_approval_status']=df['loan_approval_status'].str.lower().str.strip()\n",
        "\n",
        "# Standardising other values to either Approved, or Declined\n",
        "df['loan_approval_status']=df['loan_approval_status'].replace({'accept': 'Approved','approved': 'Approved','reject': 'Declined','declined': 'Declined'})\n",
        "\n",
        "# Remove the instance where there is no value sinces it is a part of our target variable and we dont want to risk bias by implementing the mode.\n",
        "df=df.dropna(subset=['loan_approval_status'])\n",
        "\n",
        "# Encode categorical values so they are binary.\n",
        "df['loan_approval_status']=df['loan_approval_status'].map({'Approved': 0,'Declined': 1})\n",
        "\n",
        "df['loan_approval_status'].value_counts(dropna=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "RLPSu3Cf_UVF",
        "outputId": "9875247d-8d57-47af-b651-1c79e72c5c46"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1679594251.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Make all values lowercase and removing any unwanted characters from the before and after the word.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loan_approval_status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loan_approval_status'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Standardising other values to either Approved, or Declined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All numerial values before scaling.\n",
        "df.head()\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "02n0hAALCfpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All numerical values after scaling.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "numerical_features = ['age', 'income', 'emplyment_length', 'loan_amount', 'loan_interest_rate', 'loan_income_ratio', 'credit_history_length']\n",
        "\n",
        "df[numerical_features]= scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "df.head()\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "fyFchOo07Mmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "id": "23XFWVwx_UXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the classification dataset under the name loan_approval_status_data_cleaned\n",
        "OUT_PATH = Path(\"../data/processed/loan_approval_status_data_cleaned.csv\")\n",
        "df.to_csv(OUT_PATH, index=False)"
      ],
      "metadata": {
        "id": "m-ig0hQIUSpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_loan_status = Path(\"../data/processed/loan_approval_status_data_cleaned.csv\")\n",
        "df_loan_status.head()"
      ],
      "metadata": {
        "id": "QGTesyFBBd7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Development\n",
        "\n",
        "The cleaned dataset was split into training and testing sets using an 80:20 ratio. Stratified sampling was applied to preserve class distribution across both sets.\n",
        "\n",
        "Three classification models were trained and compared:\n",
        "\n",
        "- Naïve Bayes  \n",
        "- Logistic Regression  \n",
        "- Random Forest  \n",
        "\n",
        "Model performance was evaluated using precision and recall, prioritising accurate identification of high-risk (declined) loan applications.\n"
      ],
      "metadata": {
        "id": "KA5DNS6zjrN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # The inputs are all the features on the x-axis except for the target variable\n",
        "X = df_loan_status.drop(['loan_approval_status'], axis=1)\n",
        "\n",
        "# The target variable is assigned to y\n",
        "y = df_loan_status['loan_approval_status']\n",
        "\n",
        "# Split the dataset in 80% Training and 20% Test with class stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30, stratify=y)\n",
        "\n",
        "## instantiate the model using default parameters\n",
        "# Building the models\n",
        "NB_clf = GaussianNB()\n",
        "LR_clf = LogisticRegression(random_state=30)\n",
        "RF_clf = RandomForestClassifier(random_state=30)\n",
        "\n",
        "# Train the models\n",
        "NB_clf.fit(X_train, y_train)\n",
        "LR_clf.fit(X_train, y_train)\n",
        "RF_clf.fit(X_train, y_train)\n",
        "\n",
        "# Run the model and store the predicted values\n",
        "NB_pred = NB_clf.predict(X_test)\n",
        "LR_pred = LR_clf.predict(X_test)\n",
        "RF_pred = RF_clf.predict(X_test)\n",
        "\n",
        "# View the predicted values\n",
        "NB_pred\n",
        "LR_pred\n",
        "RF_pred"
      ],
      "metadata": {
        "id": "kNi4cG7qBd_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features used for the model\n",
        "for features in X.columns:\n",
        "    print(features)\n",
        "\n",
        "# Shape of datasets\n",
        "print('\\nShape of datasets:')\n",
        "print('Whole Data shape',df_loan_status.shape)\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)"
      ],
      "metadata": {
        "id": "Bj5Ru8dgBeG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Evaluation\n",
        "\n",
        "Model performance was evaluated using business-relevant classification metrics.\n",
        "\n",
        "Due to class imbalance in the dataset, accuracy alone is not a reliable performance indicator. Instead, emphasis was placed on:\n",
        "\n",
        "- **Recall (Declined class)** – ensuring high-risk applicants are correctly identified.\n",
        "- **Precision (Declined class)** – ensuring rejected predictions are accurate.\n",
        "- Confusion matrices – to visualise true positives, false positives, false negatives, and true negatives.\n",
        "\n",
        "The Random Forest model demonstrated the strongest balance between precision and recall for identifying high-risk applicants.\n"
      ],
      "metadata": {
        "id": "dYoosCYAkBdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To plot the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# Construct the confusion matrix cm\n",
        "NB_cm = confusion_matrix(y_test, NB_pred, labels=NB_clf.classes_)\n",
        "LR_cm = confusion_matrix(y_test, LR_pred, labels=LR_clf.classes_)\n",
        "RF_cm = confusion_matrix(y_test, RF_pred, labels=RF_clf.classes_)\n",
        "\n",
        "# Create a display to plot the confusion matrix\n",
        "NB_disp = ConfusionMatrixDisplay(NB_cm,display_labels=NB_clf.classes_)\n",
        "LR_disp = ConfusionMatrixDisplay(LR_cm,display_labels=LR_clf.classes_)\n",
        "RF_disp = ConfusionMatrixDisplay(RF_cm,display_labels=RF_clf.classes_)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "NB_disp.plot()\n",
        "LR_disp.plot()\n",
        "RF_disp.plot()"
      ],
      "metadata": {
        "id": "fE5gv_-SGqpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Produce the Naive Bayes classification report for test\n",
        "print(\"Naive Bayes report \\n\", classification_report(y_test, NB_pred))\n",
        "# Plot the ROC curve for Naive Bayes clf\n",
        "NB_clf_Roc = RocCurveDisplay.from_estimator(NB_clf, X_test, y_test)\n",
        "\n",
        "# Produce the Logistic Regression classification report for test\n",
        "print(\"Logistic Regression report \\n\", classification_report(y_test, LR_pred))\n",
        "# Plot the ROC curve for Logistic Regression clf\n",
        "LR_clf_Roc = RocCurveDisplay.from_estimator(LR_clf, X_test, y_test)\n",
        "\n",
        "# Produce the Random Forest classification report for test\n",
        "print(\"Random Forest report \\n\", classification_report(y_test, RF_pred))\n",
        "# Plot the ROC curve for Random Forest clf\n",
        "RF_clf_Roc = RocCurveDisplay.from_estimator(RF_clf, X_test, y_test)"
      ],
      "metadata": {
        "id": "DU9WBiJygUHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Produce the Random Forest classification report for train\n",
        "RF_train_pred = RF_clf.predict(X_train)\n",
        "print(\"Random Forest report using train data\\n\", classification_report(y_train, RF_train_pred))\n",
        "\n",
        "# Produce the Random Forest classification report for test\n",
        "print(\"Random Forest report using test data \\n\", classification_report(y_test, RF_pred))"
      ],
      "metadata": {
        "id": "ap3KzxWN-eka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Hyperparameter Tuning\n",
        "\n",
        "To improve model performance, hyperparameter optimisation was conducted using GridSearchCV with 5-fold cross-validation.\n",
        "\n",
        "The Random Forest model was selected for tuning due to its strong baseline performance.\n",
        "\n",
        "Key parameters such as the number of estimators, maximum depth, minimum samples split, and maximum features were evaluated to enhance generalisation and improve recall for high-risk (declined) loan applications."
      ],
      "metadata": {
        "id": "ViT3HteGkldJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a new random forest classifier\n",
        "rf = RandomForestClassifier(random_state=30)\n",
        "\n",
        "#create a dictionary of all values we want to test for n_estimators\n",
        "params_rf = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20], 'min_samples_split': [2, 5], 'max_features': ['sqrt', 'log2']}\n",
        "\n",
        "#use gridsearch to test all values\n",
        "rf_gs = GridSearchCV(rf, params_rf, cv=5, n_jobs = -1)\n",
        "\n",
        "# fit model to training data\n",
        "rf_gs.fit(X_train, y_train)\n",
        "\n",
        "# save best model\n",
        "rf_best = rf_gs.best_estimator_\n",
        "\n",
        "# check best values\n",
        "print(rf_gs.best_params_)\n",
        "\n",
        "# make prediction on the test data\n",
        "y_pred_rf = rf_best.predict(X_test)"
      ],
      "metadata": {
        "id": "dp1HBiB8-en3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before tuning hyperparameters values\n",
        "params_before_tuning = {'max_depth': RF_clf.max_depth, 'max_features': RF_clf.max_features, 'min_samples_split': RF_clf.min_samples_split, 'n_estimators': RF_clf.n_estimators}\n",
        "print(params_before_tuning)"
      ],
      "metadata": {
        "id": "GxYB_3sCiYQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"confusion_matrix for RF after tuning\")\n",
        "best_rf_cm=confusion_matrix(y_test,y_pred_rf)\n",
        "disp=ConfusionMatrixDisplay(confusion_matrix=best_rf_cm,display_labels = rf_best.classes_)\n",
        "disp.plot()"
      ],
      "metadata": {
        "id": "eSTLSoft-es3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification report for RF after tuning\")\n",
        "print(classification_report(y_test,y_pred_rf))"
      ],
      "metadata": {
        "id": "2MkfP_V0-e_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Final Model Performance\n",
        "\n",
        "After hyperparameter tuning, the Random Forest model achieved strong performance on the test dataset while maintaining balanced precision and recall for the declined class.\n",
        "\n",
        "Although training performance was near-perfect, the slight performance gap between training and testing results indicates mild overfitting — a common characteristic of ensemble models.\n",
        "\n",
        "Overall, the tuned Random Forest model demonstrates strong generalisation ability and provides a reliable framework for identifying high-risk loan applicants in real-world deployment scenarios.\n"
      ],
      "metadata": {
        "id": "4yLedeHDk9wW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Ethical Considerations\n",
        "\n",
        "Automated loan approval systems must be deployed responsibly.\n",
        "\n",
        "Potential risks include:\n",
        "\n",
        "- Historical bias embedded in training data\n",
        "- Disproportionate rejection of specific demographic groups\n",
        "- Over-reliance on automated decision systems without human oversight\n",
        "\n",
        "Machine learning models should support decision-making rather than replace human judgement in high-stakes financial contexts.\n"
      ],
      "metadata": {
        "id": "DrqVROpcidj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part B – Loan Amount Prediction (Regression)\n",
        "\n",
        "## 1. Business Objective (Regression)\n",
        "\n",
        "In addition to predicting loan approval status, this section aims to predict the maximum approved loan amount using regression techniques.\n",
        "\n",
        "Accurate loan amount prediction can assist financial institutions in:\n",
        "- Risk-adjusted lending decisions\n",
        "- Capital allocation planning\n",
        "- Personalised loan offers\n"
      ],
      "metadata": {
        "id": "V-TxokF9lMYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Preparation\n",
        "\n",
        "The original dataset was reloaded to construct a regression modelling dataset.\n",
        "\n",
        "The target variable for this section is the maximum loan amount approved.\n"
      ],
      "metadata": {
        "id": "OtGGhN6blnLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "# Creating the regression dataset using the original csv file with all the data\n",
        "df2 = Path(\"../data/processed/loan_approval_status_data_cleaned.csv\")\n",
        "\n",
        "# Check values in Credit Application Acceptance\n",
        "df2['Credit_Application_Acceptance'].value_counts(dropna=False)"
      ],
      "metadata": {
        "id": "RQqiO5OPV218"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Preprocessing (Regression)\n",
        "\n",
        "Data cleaning and transformation steps were applied to prepare the dataset for regression modelling.\n",
        "\n",
        "Categorical variables were encoded and missing values were handled to ensure compatibility with regression algorithms.\n"
      ],
      "metadata": {
        "id": "ynsNhYNel8FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the instance where there is no value from Credit Application Acceptance\n",
        "df2=df2.dropna(subset=['Credit_Application_Acceptance'])\n",
        "\n",
        "# Check NaN value has been dropped successfully\n",
        "df2['Credit_Application_Acceptance'].value_counts(dropna=False)\n",
        "\n",
        "# Remove all applicants who were declined a loan.\n",
        "approved_loan_applicants =  df2[(df2.Credit_Application_Acceptance < 1)]\n",
        "\n",
        "# Checking basic stats to see if all applicants who were declined a loan have been removed successfully.\n",
        "approved_loan_applicants.describe().transpose()\n",
        "\n",
        "# Drop the Credit Application Acceptance and loan_approval_status variable from the data frame\n",
        "approved_loan_applicants.drop('loan_approval_status',axis=1, inplace=True)\n",
        "approved_loan_applicants.drop('Credit_Application_Acceptance',axis=1, inplace=True)\n",
        "#approved_loan_applicants.head()\n",
        "\n",
        "#Dropping features we don't need\n",
        "approved_loan_applicants.drop(['id'], axis=1, inplace=True)\n",
        "\n",
        "# Variable age cleaning\n",
        "approved_loan_applicants['age'] = pd.to_numeric(approved_loan_applicants['age'], errors='coerce')\n",
        "approved_loan_applicants.loc[(approved_loan_applicants['age']<18) | (approved_loan_applicants['age']>95),'age'] = None\n",
        "approved_loan_applicants = approved_loan_applicants.dropna(subset=['age'])\n",
        "\n",
        "# Variable Sex cleaning\n",
        "approved_loan_applicants=approved_loan_applicants.drop(columns=['Sex'], axis =1)\n",
        "\n",
        "# Variable Education_Qualifications cleaning\n",
        "approved_loan_applicants=approved_loan_applicants.drop(columns=['Education_Qualifications'], axis =1)\n",
        "\n",
        "# Variable home_ownership cleaning (one-hot encoding)\n",
        "approved_loan_applicants = pd.get_dummies(approved_loan_applicants, columns=['home_ownership'])\n",
        "\n",
        "# Variable emplyment_length cleaning (setting all values less than 0 and above 75 to NaN, and then removing them)\n",
        "approved_loan_applicants.loc[(approved_loan_applicants['emplyment_length']<0) | (approved_loan_applicants['emplyment_length']>75), 'emplyment_length'] = None\n",
        "approved_loan_applicants = approved_loan_applicants.dropna(subset=['emplyment_length'])\n",
        "\n",
        "# Variable loan_intent cleaning (one-hot encoding)\n",
        "approved_loan_applicants = pd.get_dummies(approved_loan_applicants, columns=['loan_intent'])\n",
        "\n",
        "# Variable loan_interest_rate cleaning (setting all values less than 0 and above 50 to NaN, and then removing them)\n",
        "approved_loan_applicants.loc[(approved_loan_applicants['loan_interest_rate']<0) | (approved_loan_applicants['loan_interest_rate']>50), 'loan_interest_rate'] = None\n",
        "approved_loan_applicants = approved_loan_applicants.dropna(subset=['loan_interest_rate'])\n",
        "\n",
        "# Variable payment_default_on_file cleaning - standardising, removing NaN values and encoding\n",
        "approved_loan_applicants['payment_default_on_file']=approved_loan_applicants['payment_default_on_file'].replace({'YES': 'Y','NO': 'N'})\n",
        "approved_loan_applicants = approved_loan_applicants.dropna(subset=['payment_default_on_file'])\n",
        "approved_loan_applicants['payment_default_on_file']=approved_loan_applicants['payment_default_on_file'].map({'Y':1, 'N':0})\n",
        "\n",
        "# Target variable max_allowed_loan cleaning\n",
        "approved_loan_applicants = approved_loan_applicants[approved_loan_applicants['max_allowed_loan'] > 0]\n",
        "approved_loan_applicants = approved_loan_applicants[approved_loan_applicants['max_allowed_loan'] < 10000000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "zbCC8GtepxGY",
        "outputId": "8fcfc49b-b76f-4be4-dd2f-b1f527f4a664"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'PosixPath' object has no attribute 'dropna'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-426780316.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Remove the instance where there is no value from Credit Application Acceptance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Credit_Application_Acceptance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Check NaN value has been dropped successfully\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Credit_Application_Acceptance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PosixPath' object has no attribute 'dropna'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "approved_loan_applicants.head(50)\n",
        "approved_loan_applicants.describe().round(2)"
      ],
      "metadata": {
        "id": "odm1DZ0f2QiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export the cleaned regression data\n",
        "save_path = Path(\"../data/processed/loan_max_amount_data.csv\")\n",
        "save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "approved_loan_applicants.to_csv(save_path, index=False)"
      ],
      "metadata": {
        "id": "oMBJvY8E2Q5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_loan_max = save_path\n",
        "df_loan_max.head()"
      ],
      "metadata": {
        "id": "-InS_T8NuDNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Dimensions of Regression Dataset', df_loan_max.shape)\n",
        "\n",
        "# Features used for the model\n",
        "for feature in df_loan_max.columns:\n",
        "    print(feature)"
      ],
      "metadata": {
        "id": "di3oQRHj4bZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# All numerical features\n",
        "numeric_features = ['age', 'income', 'emplyment_length', 'loan_amount', 'loan_interest_rate', 'loan_income_ratio', 'credit_history_length', 'max_allowed_loan']\n",
        "\n",
        "\n",
        "for features in numeric_features:\n",
        "    df_loan_max[features].hist(edgecolor='black')\n",
        "    plt.title(f'Distribution of {features}')\n",
        "    plt.xlabel(features), plt.ylabel('Frequency')\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# All encoded categorical features\n",
        "encoded_features = ['payment_default_on_file', 'home_ownership_MORTGAGE', 'home_ownership_OTHER', 'home_ownership_OWN', 'home_ownership_RENT', 'loan_intent_DEBTCONSOLIDATION', 'loan_intent_EDUCATION', 'loan_intent_HOMEIMPROVEMENT', 'loan_intent_MEDICAL', 'loan_intent_PERSONAL', 'loan_intent_VENTURE']\n",
        "\n",
        "for features in encoded_features:\n",
        "    df_loan_max[features].value_counts().plot(kind='bar', edgecolor='black'),\n",
        "    plt.title(f'Distribution of {features}'),\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8d6BlNBx_VVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_loan_max.describe()"
      ],
      "metadata": {
        "id": "L0fg02hQuPkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Model Development (Regression)\n",
        "\n",
        "This section implements regression models to predict the maximum approved loan amount.\n",
        "\n",
        "Two modelling approaches are evaluated:\n",
        "\n",
        "• Model 1 – Decision Tree Regressor using numerical features only  \n",
        "• Model 2 – Decision Tree Regressor using the full feature set (including encoded categorical variables)\n",
        "\n",
        "The dataset is split into 80% training and 20% testing data.\n",
        "\n",
        "Model performance is evaluated using regression metrics:\n",
        "- R² Score\n",
        "- Mean Absolute Error (MAE)\n",
        "- Mean Squared Error (MSE)\n",
        "\n",
        "This comparison allows assessment of whether additional categorical information improves predictive performance.\n"
      ],
      "metadata": {
        "id": "pZbNN2mrnP0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_feature = ['age', 'income', 'emplyment_length', 'loan_amount', 'loan_interest_rate', 'loan_income_ratio', 'credit_history_length']\n",
        "\n",
        "# DT Model 1\n",
        "X1 = df_loan_max[numeric_feature]\n",
        "# DT Model 2\n",
        "X2 = df_loan_max.drop(['max_allowed_loan'], axis=1)\n",
        "# Target variable for both models\n",
        "y = df_loan_max['max_allowed_loan']\n",
        "\n",
        "# Model 1 - Split the dataset in 80% Training and 20% Test\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=0.2, random_state=30)\n",
        "\n",
        "# Model 2 - Split the dataset in 80% Training and 20% Test\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.2, random_state=30)\n",
        "\n",
        "# Building Model 1\n",
        "DT1 = DecisionTreeRegressor(random_state=30)\n",
        "DT1.fit (X1_train, y1_train)\n",
        "\n",
        "# To make predictions on the test set\n",
        "y1_pred_test = DT1.predict(X1_test)\n",
        "\n",
        "# Building Model 2\n",
        "DT2 = DecisionTreeRegressor(random_state=30)\n",
        "DT2.fit (X2_train, y2_train)\n",
        "\n",
        "# To make predictions on the test set\n",
        "y2_pred_test = DT2.predict(X2_test)"
      ],
      "metadata": {
        "id": "RN10YOQY4REF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "3d0fdf02-aab7-4c1d-b47c-15c972a71280"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'PosixPath' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2487692330.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# DT Model 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_loan_max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumeric_feature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# DT Model 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_loan_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_allowed_loan'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'PosixPath' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Train-Test Split Strategy\n",
        "\n",
        "The regression dataset was divided into 80% training data and 20% testing data.\n",
        "\n",
        "The training set is used to fit the decision tree models, while the test set is used to evaluate predictive performance on unseen data.\n",
        "\n",
        "This approach helps reduce overfitting and provides a more realistic assessment of how the model would perform in real-world loan approval scenarios.\n"
      ],
      "metadata": {
        "id": "3H_PL8tdxNpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DT Model 1s dimensions\n",
        "print(\"Model 1 - Numeric features only\")\n",
        "print(\"X1_train shape:\", X1_train.shape)\n",
        "print(\"X1_test shape:\", X1_test.shape)\n",
        "print(\"y_train shape:\", y1_train.shape)\n",
        "print(\"y_test shape:\", y1_test.shape)\n",
        "\n",
        "# DT Model 1s features\n",
        "print(\"\\nModel 1 features:\")\n",
        "for features in X1.columns:\n",
        "  print(features)"
      ],
      "metadata": {
        "id": "rlltmnbH2GLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DT Model 2s dimensions\n",
        "print(\"Model 2 - All retained features\")\n",
        "print(\"X2_train shape:\", X2_train.shape)\n",
        "print(\"X2_test shape:\", X2_test.shape)\n",
        "print(\"y_train shape:\", y2_train.shape)\n",
        "print(\"y_test shape:\", y2_test.shape)\n",
        "\n",
        "# DT Model 2s features\n",
        "print(\"\\nModel 2 features:\")\n",
        "for features in X2.columns:\n",
        "  print(features)"
      ],
      "metadata": {
        "id": "qel_80FT2ynd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Feature Importance\n",
        "\n",
        "To understand which variables most influence maximum approved loan amount, feature importance from the decision tree model was examined.\n",
        "\n",
        "Income, loan amount requested, and credit history length appear to be the strongest predictors.\n",
        "\n",
        "This aligns with financial intuition, as lenders prioritise repayment capacity and borrowing history when determining maximum lending thresholds.\n"
      ],
      "metadata": {
        "id": "OBYwNYM6xdSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Metrics for Model 1')\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y1_test, y1_pred_test))\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y1_test, y1_pred_test))\n",
        "print('R2:', metrics.r2_score(y1_test, y1_pred_test))\n",
        "\n",
        "print('\\nMetrics for Model 2')\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y2_test, y2_pred_test))\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y2_test, y2_pred_test))\n",
        "print('R2:', metrics.r2_score(y2_test, y2_pred_test))"
      ],
      "metadata": {
        "id": "RTg3TE_J1P4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Model Comparison\n",
        "\n",
        "Two regression models were evaluated:\n",
        "\n",
        "• Model 1 – Numerical features only  \n",
        "• Model 2 – Full feature set (including encoded categorical variables)\n",
        "\n",
        "Model 1 achieved:\n",
        "- R² ≈ 0.969  \n",
        "- MAE ≈ £1,210  \n",
        "\n",
        "Model 2 achieved:\n",
        "- R² ≈ 0.971  \n",
        "- MAE ≈ £1,257  \n",
        "\n",
        "Both models demonstrate strong predictive performance.\n",
        "\n",
        "The marginal improvement in R² suggests that numerical financial indicators (income, loan amount, credit history length) capture most of the predictive signal.\n",
        "\n",
        "Given the minimal performance gain, the simpler feature set may be preferable for efficiency and interpretability.\n"
      ],
      "metadata": {
        "id": "8BXysFO-wfI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the tree growth to 4 levels\n",
        "DT1_pruned_regressor = DecisionTreeRegressor(max_depth=4, random_state=30)\n",
        "DT1_pruned_regressor.fit(X1_train, y1_train)\n",
        "\n",
        "# To make predictions on the test set\n",
        "y1_pred_pruned = DT1_pruned_regressor.predict(X1_test)\n",
        "\n",
        "# Plot the regression DT\n",
        "pruned_Tree_model = plt.figure(figsize=(20,10))\n",
        "pruned_Tree_model_Graph = tree.plot_tree(DT1_pruned_regressor, feature_names=list(X1_train.columns), filled=True)\n",
        "\n",
        "# To save the DT graph as a png image\n",
        "pruned_Tree_model.savefig(\"pruned_reg_decision_tree.png\")\n",
        "\n",
        "# Calculating the regression metrics for the pruned regression decision Tree\n",
        "print ('Metrics for Pruned Regression Decision Tree')\n",
        "print('Mean Squared Error:', metrics.mean_squared_error(y1_test, y1_pred_pruned))\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y1_test, y1_pred_pruned))\n",
        "print('R2:', metrics.r2_score(y1_test, y1_pred_pruned))\n",
        "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y1_test, y1_pred_pruned)))"
      ],
      "metadata": {
        "id": "pORgVX7MpgJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Effect of Tree Pruning\n",
        "\n",
        "Restricting the tree depth to four levels reduced model complexity and improved interpretability.\n",
        "\n",
        "However, performance declined significantly (R² ≈ 0.86), indicating that deeper splits were capturing important interaction effects.\n",
        "\n",
        "This highlights the trade-off between model simplicity and predictive power. In regulated financial environments, simpler models may be preferred for transparency, but at the cost of reduced accuracy.\n"
      ],
      "metadata": {
        "id": "2eBcqnlcwlRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame from scratch to predict Maximum Loan Amount\n",
        "data = []\n",
        "data.append( {\"age\":56,\n",
        "              \"income\":57000,\n",
        "              \"emplyment_length\":15,\n",
        "              \"loan_amount\":25700,\n",
        "              \"loan_interest_rate\":23,\n",
        "              \"loan_income_ratio\":0.10,\n",
        "              \"credit_history_length\":35,\n",
        "              } )\n",
        "df3 = pd.DataFrame(data)\n",
        "\n",
        "# Add a new column to `df3` with the predicted prices:\n",
        "df3[\"Predicted Max Loan Amount\"] = DT1_pruned_regressor.predict(df3)\n",
        "df3.head()"
      ],
      "metadata": {
        "id": "7sJM4EGm-AF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. New Applicant Prediction\n",
        "\n",
        "For a 56-year-old applicant with:\n",
        "- Income: £57,000  \n",
        "- Employment length: 15 years  \n",
        "- Loan amount requested: £25,700  \n",
        "- Interest rate: 23%  \n",
        "- Credit history length: 35 years  \n",
        "\n",
        "The pruned model predicts a maximum approved loan amount of approximately **£92,901**.\n",
        "\n",
        "This demonstrates how the regression model can support lending decision simulations and scenario analysis in real-world banking environments.\n"
      ],
      "metadata": {
        "id": "K1EidqWcwouS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Limitations\n",
        "\n",
        "Although the Decision Tree model performs well, several limitations exist:\n",
        "\n",
        "- Decision trees can overfit complex datasets.\n",
        "- The model assumes historical approval patterns remain stable over time.\n",
        "- External macroeconomic factors (e.g., inflation, interest rate shifts) are not incorporated.\n",
        "\n",
        "Future work could explore ensemble methods such as Random Forest or Gradient Boosting to improve robustness.\n"
      ],
      "metadata": {
        "id": "szqrnHNiw_J1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Conclusion – Regression Analysis\n",
        "\n",
        "The Decision Tree Regressor demonstrates strong predictive performance for estimating maximum approved loan amounts.\n",
        "\n",
        "Key findings:\n",
        "- Numerical financial indicators drive most predictive power.\n",
        "- Categorical variables provide limited additional improvement.\n",
        "- Deep trees increase accuracy but reduce interpretability.\n",
        "- Pruned trees simplify structure at the cost of performance.\n",
        "\n",
        "Overall, regression modelling provides valuable decision support for capital allocation and personalised lending strategies.\n"
      ],
      "metadata": {
        "id": "ofXaVYiGwq3S"
      }
    }
  ]
}